# ğŸ‘‹ Hi, I'm Nivesh Gurjar

ğŸš€ **AWS Data Engineer | Python | PySpark | SQL | Glue, S3, Lambda, Athena**

I'm a cloud-first data engineer with 4+ years of experience building scalable, serverless ETL pipelines using AWS services and PySpark. I specialise in automating data workflows, optimizing query performance, and architecting production-ready data lake solutions.

---

## ğŸ”§ Tech Stack & Tools

- ğŸ Python, PySpark, SQL
- â˜ï¸ **AWS:** Glue, S3, Lambda, Athena, Step Functions, IAM, CloudWatch
- ğŸ”¥ Databricks, Delta Lake, Apache Spark
- ğŸ› ï¸ Control-M, Git, Jenkins, REST APIs
- ğŸ“Š PostgreSQL, MySQL, Pandas

---

## ğŸš€ Featured Projects

### ğŸŒ€ **Serverless Flight Delay ETL (AWS Glue + Athena)**
Designed and deployed an AWS Glue-based ETL pipeline to process flight delay datasets and store them in partitioned S3 buckets, queried via Athena.

### ğŸ” **Secure PII Data Masking (PySpark + S3)**
Implemented a scalable data masking framework to anonymise PII across large datasets using PySpark and stored results securely in S3 with metadata tagging.

### ğŸ§¹ **S3 Data Cleanup Automation (Lambda + Boto3)**
Built an AWS Lambda function to automate the cleanup of unused files in S3 based on lifecycle rules, integrated with CloudWatch alerts.

### ğŸ“Š **SQL Optimization Portfolio**
Advanced SQL queries using CTEs, window functions, and indexing. Includes performance comparison before and after optimization.

---

## ğŸ“ˆ GitHub Stats

<p align="center">
  <img src="https://github-readme-stats.vercel.app/api?username=niveshgurjar&show_icons=true&theme=tokyonight" alt="GitHub Stats" />
  <img src="https://github-readme-stats.vercel.app/api/top-langs/?username=niveshgurjar&layout=compact&theme=tokyonight" alt="Top Languages" />
</p>

---

## ğŸ“« Connect with Me

- ğŸ”— [LinkedIn](https://linkedin.com/in/your-linkedin)  
- ğŸ“§ Email: your.email@gmail.com  
- ğŸŒ GitHub: [github.com/niveshgurjar](https://github.com/niveshgurjar)

---

_â˜ï¸ Building smart, secure, and scalable data solutions on the cloud â€” one pipeline at a time._
